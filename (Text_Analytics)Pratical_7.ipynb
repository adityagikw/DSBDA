{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkv_U3gaQgEk"
      },
      "source": [
        "Text Analytics\n",
        "\n",
        "1. Extract Sample document and apply following document preprocessing methods:\n",
        "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
        "2. Create representation of document by calculating Term Frequency and Inverse Document\n",
        "Frequency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dBm0mVf9QbK7"
      },
      "outputs": [],
      "source": [
        "import nltk # The Natural Language Toolkit library, which provides functions for text processing.\n",
        "from nltk import pos_tag # Function to perform Part-of-Speech tagging.\n",
        "from nltk.tokenize import TreebankWordTokenizer # A tokenizer that splits text into words.\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer # A stemmer that reduces words to their base or root form (e.g., \"running\" → \"run\") A lemmatizer that reduces words to their dictionary form (e.g., \"running\" → \"run\").\n",
        "from nltk.corpus import stopwords # A list of common words (like \"the\", \"is\") that are generally ignored in text analysis.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Converts a collection of text documents to a matrix of Term Frequencies and Inverse Document Frequencies (TF-IDF)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoQ-Hy8_QlsI",
        "outputId": "ed6380e4-a13f-45cd-e5f6-6408adb9884b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# nltk.download(): Downloads necessary datasets and models.\n",
        "\n",
        "nltk.download('stopwords') # stopwords: Commonly used words in text that should be removed.\n",
        "nltk.download('punkt') # punkt: A tokenizer used by NLTK to split text into words.\n",
        "nltk.download('wordnet') # wordnet: A lexical database for the English language.\n",
        "nltk.download('averaged_perceptron_tagger_eng') # averaged_perceptron_tagger_eng: A part-of-speech tagger for English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GDgv8wqsQ1O-"
      },
      "outputs": [],
      "source": [
        "document = \"My name is  Aditya Rajendra Gaikwad. We can process the text with the natural language processing is better Aditya\"\n",
        "# document: A sample text that will be processed. It contains a sentence with a name, common words, and repeated words to showcase text preprocessing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp9eNZsLRDSi",
        "outputId": "bc461127-91c2-40ae-d8b5-5f7cfb8ea989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['My', 'name', 'is', 'Aditya', 'Rajendra', 'Gaikwad.', 'We', 'can', 'process', 'the', 'text', 'with', 'the', 'natural', 'language', 'processing', 'is', 'better', 'Aditya']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = TreebankWordTokenizer() # Initializes the tokenizer, which splits text into words (tokens) based on whitespace and punctuation.\n",
        "tokens = tokenizer.tokenize(document) # Tokenizes the sample document into individual words and punctuation marks.\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_db8jjfoRHuq",
        "outputId": "4ad60583-3dbd-4c33-b387-6e4eb7655585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'been', 'was', 'be', 'does', \"we'll\", 'by', \"shouldn't\", 'on', 'against', 'where', \"it'll\", \"shan't\", \"he'd\", 'their', \"they've\", 'd', 'this', 'you', 'yourself', 'myself', 'yours', 'with', 'own', 'ourselves', 'mightn', 'weren', 'his', 'isn', 'did', 'doing', 'such', 'those', \"hadn't\", 'were', 'why', 'between', \"doesn't\", 'itself', \"haven't\", 'he', 'through', 'ours', 'any', 'had', 'him', 'before', \"they'll\", 'whom', 'further', 'from', \"they'd\", 'wouldn', 'then', 'again', 'off', 'there', \"you'll\", 'won', \"weren't\", 'so', 'haven', 'shouldn', \"wasn't\", 'it', 'that', 'am', 'now', 'of', \"isn't\", 'when', 'a', 'wasn', 'hadn', 'your', 'very', \"i'd\", 'up', 'o', 'same', \"didn't\", 'she', 'what', 're', \"we'd\", \"won't\", 'will', 'until', 'not', 'more', 'doesn', \"you'd\", 'nor', \"i'm\", \"should've\", \"mightn't\", 'yourselves', \"she'll\", \"we're\", 'because', 'm', \"aren't\", \"he'll\", \"couldn't\", 'down', 'having', 'needn', 'themselves', 'these', \"she'd\", 'are', \"wouldn't\", 'couldn', 'if', 'or', 'himself', \"it's\", 'once', 'theirs', \"i've\", 'most', 'some', 'her', 'hasn', 'they', 'is', 'how', 'in', \"don't\", 'too', 'during', 'mustn', 'being', 'should', 'we', \"we've\", 'ain', 'don', 's', 'only', 'i', 'aren', \"he's\", 'to', 'has', \"mustn't\", 'herself', 'hers', 'few', 'ma', 'can', \"you've\", 'over', 'each', 'which', 'into', 'shan', 'll', 've', 'at', \"they're\", 'y', \"needn't\", \"it'd\", \"you're\", \"hasn't\", 'after', 'here', 'other', 'no', 'both', 'for', \"she's\", 'above', 'just', 'and', 'our', 'out', 'an', 'the', 'didn', 'below', 'do', 'its', 'me', 'all', 'under', 'them', \"that'll\", 'than', 'who', 'as', 'while', 't', 'but', \"i'll\", 'about', 'have', 'my'}\n"
          ]
        }
      ],
      "source": [
        "stp_words = set(stopwords.words('english')) # stopwords.words('english'): Retrieves a list of common English stop words (e.g., \"the\", \"is\", \"in\"). set(stp_words): Converts the list of stopwords into a set for faster lookup.\n",
        "print(stp_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNYLMJSzRKr7",
        "outputId": "79faa25f-5227-4d25-b3e9-10084e0e4ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Aditya', 'NNP'), ('Rajendra', 'NNP'), ('Gaikwad.', 'NNP'), ('We', 'PRP'), ('can', 'MD'), ('process', 'VB'), ('the', 'DT'), ('text', 'NN'), ('with', 'IN'), ('the', 'DT'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('better', 'RBR'), ('Aditya', 'NNP')]\n"
          ]
        }
      ],
      "source": [
        "pos = pos_tag(tokens) # pos_tag(tokens): Tags each word with its part of speech (e.g., noun, verb, adjective) based on the word's usage in the sentence.\n",
        "print(pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IHTeZK4RNfE",
        "outputId": "e354ed64-9857-432b-f7f4-4194e19d5152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['name', 'Aditya', 'Rajendra', 'Gaikwad.', 'process', 'text', 'natural', 'language', 'processing', 'better', 'Aditya']\n"
          ]
        }
      ],
      "source": [
        "filtered = [word for word in tokens if word.lower() not in stp_words] # filtered: Creates a list of tokens that are not stopwords by checking if the word (converted to lowercase) is not in the stopwords list.\n",
        "print(filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMHEInSHRQV8",
        "outputId": "a14c0f07-bcf4-4434-f7b5-c1dba35d1560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['name', 'aditya', 'rajendra', 'gaikwad.', 'process', 'text', 'natur', 'languag', 'process', 'better', 'aditya']\n"
          ]
        }
      ],
      "source": [
        "stemmer = PorterStemmer() # PorterStemmer(): Initializes the stemmer.\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered] # stemmer.stem(word): Applies the Porter stemming algorithm to reduce words to their root form. stemmed_tokens: A list of tokens after stemming.\n",
        "print(stemmed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM30PJW8RSlQ",
        "outputId": "6f2bbc08-b8c6-495e-9ef7-e31e72298561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['name', 'Aditya', 'Rajendra', 'Gaikwad.', 'process', 'text', 'natural', 'language', 'processing', 'better', 'Aditya']\n"
          ]
        }
      ],
      "source": [
        "lemma = WordNetLemmatizer() # WordNetLemmatizer(): Initializes the lemmatizer.\n",
        "lemma_tokens = [lemma.lemmatize(word) for word in filtered] # lemma.lemmatize(word): Applies the WordNet lemmatization algorithm to reduce words to their base form. lemma_tokens: A list of lemmatized tokens.\n",
        "print(lemma_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wOC-lpqRUve",
        "outputId": "cc1d18a9-0595-4b08-cb77-aa1799aadfb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name Aditya Rajendra Gaikwad. process text natural language processing better Aditya\n"
          ]
        }
      ],
      "source": [
        "process_tokens = \" \".join(lemma_tokens) # \" \".join(lemma_tokens): Joins the lemmatized tokens back into a single string, forming the preprocessed document.\n",
        "print(process_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hW2o_x0lRX_t"
      },
      "outputs": [],
      "source": [
        "documents = [process_tokens] # documents: A list containing the preprocessed document (in this case, only one document). This is used for computing the TF-IDF values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwzclxgKRa51",
        "outputId": "e99ce8a8-d08d-49f1-c8ad-ce1c92a293af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "feature aditya : value 0.5547\n",
            "feature better : value 0.2774\n",
            "feature gaikwad : value 0.2774\n",
            "feature language : value 0.2774\n",
            "feature name : value 0.2774\n",
            "feature natural : value 0.2774\n",
            "feature process : value 0.2774\n",
            "feature processing : value 0.2774\n",
            "feature rajendra : value 0.2774\n",
            "feature text : value 0.2774\n"
          ]
        }
      ],
      "source": [
        "vector = TfidfVectorizer() # TfidfVectorizer(): Initializes the TF-IDF vectorizer, which converts a collection of text documents into a matrix of TF-IDF features.\n",
        "tfidf_mat = vector.fit_transform(documents) # vector.fit_transform(documents): Computes the TF-IDF matrix for the provided documents.\n",
        "name = vector.get_feature_names_out() # vector.get_feature_names_out(): Retrieves the list of feature names (words) from the TF-IDF model.\n",
        "value = tfidf_mat.toarray() # tfidf_mat.toarray(): Converts the TF-IDF matrix into a NumPy array.\n",
        "for word,val in zip(name,value[0]):\n",
        "    print(f\"feature {word} : value {val:.4f}\")\n",
        "\n",
        "    #zip(name, value[0]): Pairs each word (feature) with its corresponding TF-IDF value. print(f\"feature {word} : value {val:.4f}\"): Prints each feature (word) and its corresponding TF-IDF value, rounded to 4 decimal places. This helps understand the significance of each word in the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP5fpd2LG2pc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
